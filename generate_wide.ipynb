{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import os\n",
    "import numpy\n",
    "import datetime as datetime\n",
    "import boto3\n",
    "from io import BytesIO\n",
    "\n",
    "#replace path with correct path\n",
    "os.chdir(\"/home/ec2-user/Contacts-sensitive/\")\n",
    "import functions.aws_um_funcs as funcs\n",
    "#from functions.aws_um_funcs import *\n",
    "\n",
    "#from meat_packers.mp_alt import *\n",
    "from meat_packers.turnover import *\n",
    "from meat_packers.mp_alt_custom import *\n",
    "import botocore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal:\n",
    "# Automatically generate wide format data for all packers, indexed by date\n",
    "#\n",
    "# - Iterate thru mpdata, if valid, read from s3\n",
    "# - Create new df indexed by date in given date range, 2nd column packer, 3rd column fips\n",
    "# - filter to only connections at the facility\n",
    "# - add columns for: #devices/day, #new devices/day, #separations, *add more as necessary\n",
    "# - concat this to large df\n",
    "# - sort by date\n",
    "# - write df to s3 in some format, parquet? (do when rest working)\n",
    "#\n",
    "#\n",
    "# Questions/Considerations: \n",
    "# What time period to consider for separation? default infinity\n",
    "# Calculate turnover now and put in this df, or use this df to calculate turnover later?\n",
    "# Add Covid rates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sum_stats(mp_path, start_date, end_date):\n",
    "    \"\"\"\n",
    "    generates the summary dataframe for all packers. saves each one to s3 to be combined later\n",
    "    \"\"\"\n",
    "    mpdata = get_mp_cleaned(mp_path)\n",
    "    need_dates = []\n",
    "        \n",
    "    if end_date > date(2020,8,24):\n",
    "        need_dates1 = funcs.daterange(date(2020,1,1), date(2020,8,24))\n",
    "        need_dates2 = funcs.daterange(date(2020,8,26), end_date)\n",
    "        need_dates = [*need_dates1, *need_dates2]\n",
    "    else:\n",
    "        need_dates = funcs.daterange(date(2020,1,1), end_date)\n",
    "    \n",
    "    s3 = boto3.resource('s3')\n",
    "    \n",
    "    for mp_index in range(0,404):\n",
    "        \n",
    "        try:\n",
    "            existing_s3_key = 'meat_packers/summary/'+ str(mp_index) + '.parquet'\n",
    "            s3.Object('yse-bioecon', existing_s3_key).load()\n",
    "            print(f\"{mp_index} already exists\")\n",
    "            continue\n",
    "        except:\n",
    "            print(f\"generating {mp_index}\")\n",
    "        mp_row = mpdata[mp_index:mp_index+1]\n",
    "        \n",
    "        company_name = str(mp_index) + \"_\" + mp_row[\"company\"].iloc[0].replace(\" \", \"_\")\n",
    "        fips_code = mp_row[\"geoid\"].iloc[0]\n",
    "        s3_key = 'meat_packers/contacts-dc/'+ fips_code + \"/\" + company_name + '.parquet'\n",
    "\n",
    "        try:\n",
    "            s3.Object('yse-bioecon', s3_key).load()\n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if e.response['Error']['Code'] == \"404\":\n",
    "                print(f\"skipping packer, {s3_key} does not exist\")\n",
    "                # The object does not exist.\n",
    "                \n",
    "            else:\n",
    "                # Something else has gone wrong.\n",
    "                print(\"skipping packer\")\n",
    "                \n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            # The object does exist.\n",
    "            packer_poly = mp_poly(mp_row)\n",
    "            \n",
    "            if len(packer_poly) <= 0:\n",
    "                continue\n",
    "            packer_df = generate_packer_df(s3_key, need_dates, packer_poly.iloc[0][\"address\"], packer_poly, company_name, fips_code)\n",
    "            try:\n",
    "                my_buffer = BytesIO()\n",
    "                packer_df.to_parquet(my_buffer)\n",
    "                s3.Object('yse-bioecon', 'meat_packers/summary/' + str(mp_index) + '.parquet').put(Body=my_buffer.getvalue())\n",
    "            except:\n",
    "                print(\"no contacts detected\")\n",
    "    return\n",
    "\n",
    "def create_wide_df(mp_path):\n",
    "    \"\"\"\n",
    "    combines individual summary files into one large dataframe\n",
    "    \"\"\"\n",
    "    s3 = boto3.resource('s3')\n",
    "       \n",
    "    dfs = []\n",
    "    for i in range(404):\n",
    "       \n",
    "\n",
    "        s3_key = 'meat_packers/summary/' + str(i) + '.parquet'\n",
    "        try:\n",
    "            s3.Object('yse-bioecon', s3_key).load()\n",
    "            data = getpardata('yse-bioecon', s3_key)\n",
    "            dfs.append(data)\n",
    "        except:\n",
    "            continue\n",
    "    wide = pd.concat(dfs) if len(dfs) > 1 else dfs[0]\n",
    "    \n",
    "       \n",
    "    wide = wide.reset_index()\n",
    "    wide = wide.sort_values([\"date_dt\"]).reset_index(drop=True)\n",
    "    \n",
    "    return wide\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mp_cleaned(mp_path):\n",
    "    mpdata = pd.read_csv(mp_path)\n",
    "    mpdata = mpdata[mpdata['verified'] == 1]\n",
    "    mpdata = mpdata[mpdata['close_to'] == 0]\n",
    "    mpdata = mpdata[mpdata['polyid'].notna()]\n",
    "    mpdata = mpdata.drop(['verified', 'close_to', 'census_tract.y'], axis = 1)\n",
    "    mpdata = mpdata.rename(columns = {'census_tract.x' : 'census_tract'})\n",
    "    mpdata['geoid'] = mpdata['fips_code'].apply(lambda x: (\"0\" + str(x)) if len(str(x)) == 4 else str(x))\n",
    "    mpdata['path'] = mpdata['state'].apply(lambda x: x.lower()) + \"/\" + mpdata['geoid'] + \"/\"\n",
    "    mpdata.reset_index(drop = True, inplace =True)\n",
    "    #make mpdata spatial\n",
    "    mpdata = gpd.GeoDataFrame(mpdata, geometry = gpd.points_from_xy(mpdata.lon, mpdata.lat)).set_crs(epsg=4326)\n",
    "    return mpdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO split this into more functions, make more efficient. Current implementation is slow\n",
    "def generate_packer_df(s3_key, need_dates, address, packer_poly, company_name, fips_code):\n",
    "    \n",
    "    \"\"\"\n",
    "    generates summary data for a given packer\n",
    "    total_contacts, different_fips, num_contacts_packer, unique_devices_packer, new_devices, separations, mean_cel, std_cel\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    data = getpardata('yse-bioecon', s3_key)\n",
    "   \n",
    "    if len(data) <= len(need_dates):\n",
    "        print(company_name)\n",
    "        print(\"abort!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    data[\"index0\"] = range(len(data))\n",
    "    \n",
    "    #TODO error, to_datetime fails on a few packers, not sure why\n",
    "    try:\n",
    "        data[\"date_dt\"] = pd.to_datetime(data[\"date\"], format='%Y%m%d')\n",
    "    except:\n",
    "        return\n",
    "    \n",
    "    #total contacts, different fips, contacts at packer\n",
    "    gb_date_full = data.groupby([\"date_dt\"])[\"device_id_1\"]\n",
    "    total_contacts = gb_date_full.count().reset_index().rename(columns={'device_id_1': 'total_contacts'}).set_index('date_dt')\n",
    "    \n",
    "    data_diff_fips = data[data.fips_code != str(fips_code)]\n",
    "    gb_date_fips = data_diff_fips.groupby([\"date_dt\"])[\"device_id_1\"]\n",
    "    different_fips = gb_date_fips.count().reset_index().rename(columns={'device_id_1': 'different_fips'}).set_index('date_dt')\n",
    "    \n",
    "    at_packer = data[data.address == address]\n",
    "    gb_date = at_packer.groupby([\"date_dt\"])[\"device_id_1\"]\n",
    "    num_contacts_packer = gb_date.count().reset_index().rename(columns={'device_id_1': 'at_packer'}).set_index('date_dt')\n",
    "\n",
    "    \n",
    "    #empty dataframes to be filled, can remove most of these but left in for now\n",
    "    date_df = pd.DataFrame()\n",
    "    unique_devices_packer = pd.DataFrame()\n",
    "    new_devices = pd.DataFrame()\n",
    "    separations = pd.DataFrame()\n",
    "    mean_cel = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    #common evening locations\n",
    "    std_cel = pd.DataFrame()\n",
    "    cel_series = pd.concat([data[[\"date_dt\", \"cel_distance_m_1\"]].rename(columns={'cel_distance_m_1': 'cel_distance'}), \n",
    "                            data[[\"date_dt\", \"cel_distance_m_2\"]].rename(columns={'cel_distance_m_2': 'cel_distance'})])\n",
    "\n",
    "    cel_gb = cel_series.groupby([\"date_dt\"])\n",
    "    mean_cel = cel_gb.mean().rename(columns={'cel_distance': 'mean_cel'})\n",
    "    std_cel = cel_gb.std().rename(columns={'cel_distance': 'std_cel'})\n",
    "    \n",
    "    \n",
    "    \n",
    "    #duplicates each contact into one row with device 1, one row with device 2\n",
    "    long_data = pd.wide_to_long(at_packer, [\"device_id_\"], i='index0', j=\"test\")\n",
    "    \n",
    "    \n",
    "    #unique devices at the packing facility\n",
    "    gb_date_long = long_data.groupby([\"date_dt\"])[\"device_id_\"]\n",
    "    unique_devices_packer = gb_date_long.nunique().reset_index().rename(columns={'device_id_': 'unique_devices'})\n",
    "    unique_devices_packer.set_index('date_dt', inplace=True)\n",
    "       \n",
    "    #calculate new devices at packer    \n",
    "    #days_since_last_visit = long_data.sort_values(['device_id_', 'date_dt']).groupby('device_id_')['date_dt'].diff().dt.days\n",
    "    days_since_last_visit = long_data.sort_values(['device_id_', 'date_dt']).groupby('device_id_')['date_dt'].diff()\n",
    "    asc_data = long_data.sort_values([\"device_id_\", \"date_dt\"])\n",
    "    asc_data[\"days_since_last_appearance\"] = days_since_last_visit\n",
    "    asc_data = asc_data[asc_data[\"days_since_last_appearance\"].isna()].drop(['usecode', 'neighborhood', 'zoning', 'numstories', 'sqft', 'rdi',\n",
    "           'unixtime_2', 'speed_kph_1', 'census_block', 'parval',\n",
    "           'numunits', 'cel_distance_m_2', 'saleprice',\n",
    "           'local_time_1', 'lat', 'geoid', 'structstyle', 'unixtime_1', 'mailadd',\n",
    "           'gisacre', 'agval', 'usedesc', 'mail_state2',\n",
    "           'local_time_2', 'cbg', 'scity', 'usps_vacancy_date', 'lbcs_activity',\n",
    "           'lbcs_function', 'parcelnumb', 'lbcs_function_desc', 'index_right',\n",
    "           'census_blockgroup', 'dpv_status', 'll_gissqft', 'datasource_id_2',\n",
    "           'cel_distance_m_1', 'mail_city', 'yearbuilt', 'distance_m',\n",
    "           'lbcs_activity_desc', 'census_tract', 'owntype', 'parvaltype', 'city',\n",
    "           'szip', 'lon', 'll_gisacre', 'mail_zip', 'speed_kph_2',\n",
    "           'geometry', 'zoning_description', 'usps_vacancy', 'saledate',\n",
    "           'datasource_id_1', 'date', 'address', 'days_since_last_appearance', 'crap', \"fips_code\"], axis=1)\n",
    "    \n",
    "    new_devices = asc_data.groupby('date_dt').count().reset_index().rename(columns={'device_id_': 'new_devices'}).reset_index(drop=True)\n",
    "    new_devices.set_index('date_dt', inplace=True)\n",
    "\n",
    "    #calculate separations\n",
    "    #days_to_next_visit = long_data.sort_values(by=['device_id_', 'date_dt'], ascending=False).groupby('device_id_')['date_dt'].diff().dt.days\n",
    "    days_to_next_visit = long_data.sort_values(by=['device_id_', 'date_dt'], ascending=False).groupby('device_id_')['date_dt'].diff()\n",
    "\n",
    "    dsc_data = long_data.sort_values([\"device_id_\", \"date_dt\"], ascending=False)\n",
    "    dsc_data[\"days_to_next_visit\"] = days_to_next_visit\n",
    "    \n",
    "    dsc_data = dsc_data[dsc_data[\"days_to_next_visit\"].isna()].drop(['usecode', 'neighborhood', 'zoning', 'numstories', 'sqft', 'rdi',\n",
    "           'unixtime_2', 'speed_kph_1', 'census_block', 'parval',\n",
    "           'numunits', 'cel_distance_m_2', 'saleprice',\n",
    "           'local_time_1', 'lat', 'geoid', 'structstyle', 'unixtime_1', 'mailadd',\n",
    "           'gisacre', 'agval', 'usedesc', 'mail_state2',\n",
    "           'local_time_2', 'cbg', 'scity', 'usps_vacancy_date', 'lbcs_activity',\n",
    "           'lbcs_function', 'parcelnumb', 'lbcs_function_desc', 'index_right',\n",
    "           'census_blockgroup', 'dpv_status', 'll_gissqft', 'datasource_id_2',\n",
    "           'cel_distance_m_1', 'mail_city', 'yearbuilt', 'distance_m',\n",
    "           'lbcs_activity_desc', 'census_tract', 'owntype', 'parvaltype', 'city',\n",
    "           'szip', 'lon', 'll_gisacre', 'mail_zip', 'speed_kph_2',\n",
    "           'geometry', 'zoning_description', 'usps_vacancy', 'saledate',\n",
    "           'datasource_id_1', 'date', 'address', 'days_to_next_visit', 'crap', \"fips_code\"], axis=1)\n",
    "   \n",
    "    \n",
    "    separations = dsc_data.sort_values([\"date_dt\"]).groupby('date_dt').count().reset_index().rename(columns={'device_id_': 'separations'}).reset_index(drop=True)\n",
    "    separations.set_index('date_dt', inplace=True)\n",
    "    \n",
    "    \n",
    "\n",
    "    #setup dataframe to return\n",
    "    date_df = pd.DataFrame({'date_dt': need_dates})\n",
    "    date_df[\"date_dt\"] = pd.to_datetime(date_df[\"date_dt\"], format='%Y%m%d')\n",
    "    date_df = date_df.set_index([\"date_dt\"])\n",
    "    \n",
    "    result = pd.concat([total_contacts, different_fips, num_contacts_packer, unique_devices_packer, new_devices, separations, date_df, mean_cel, std_cel], join=\"outer\", axis=1).fillna(0)\n",
    "    result[\"fips_code\"] = fips_code\n",
    "    result[\"company_name\"] = company_name\n",
    "    while 'use' in result.columns:\n",
    "        result = result.drop(['use'], axis=1)\n",
    "    print('success')\n",
    "    return result\n",
    "\n",
    "def mp_poly(mpdata):\n",
    "    #copied from mp_alt\n",
    "    #select a row from the packer, mpdata, geodataframe\n",
    "    tmp = mpdata[0:1]\n",
    "\n",
    "    \n",
    "    #get the gpkg file -- this needs to be seperate because it is slow, and we can group this stream. \n",
    "    tmp_geoid = tmp['geoid'].iloc[0]\n",
    "    tmp_gpkg =  funcs.getgpkg_gz('yse-bioecon', 'landgrid_geoid/' + tmp_geoid + \".gpkg.gz\")\n",
    "\n",
    "    #Gets the polygon associated with a single meatpacker\n",
    "    packer_tmp = get_packer_poly(tmp, tmp_gpkg)\n",
    "    return packer_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sum_stats(\"s3://yse-bioecon/meat_packers/meat_packers.csv\", date(2020,1,1), date(2021,4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = create_wide_df(\"s3://yse-bioecon/meat_packers/meat_packers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[test.company_name != \"5_DEAN_SAUSAGE_CO_INC\"] #this one never has any contacts, slipped through filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.describe()\n",
    "#total_contacts: how many contacts workers from a given packer made in a day\n",
    "#different_fips: how many of said contacts were in a different county from the packing facility\n",
    "#at_packer: how many contacts were at the facility\n",
    "#unique devices: how many unique devices were at the facility\n",
    "#new devices: number of devices at facility which have never beenn seen there before\n",
    "#separations: number of devices at facility which are never seen again\n",
    "#mean cel: average distance from common evening location of all devices\n",
    "#std cel: standard deviation of above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gb_date = test.groupby([\"date_dt\"])\n",
    "ratio = test_gb_date.different_fips.mean() / test_gb_date.total_contacts.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[test. <= 20000].total_contacts.hist(bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[[\"total_contacts\", \"different_fips\", \"at_packer\"]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[[\"mean_cel\"]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gb_date.total_contacts.mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt2020 = pd.read_csv('https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties-2020.csv')\n",
    "nyt2021 = pd.read_csv('https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties-2021.csv')\n",
    "nytcovid = pd.concat([nyt2020, nyt2021]).rename(columns={'fips': 'fips_code'}).drop([\"state\", \"county\"], axis=1)\n",
    "nytcovid['fips_code'] = nytcovid['fips_code'].apply(lambda x: (\"0\" + str(x))[:-2] if len(str(x)) == 6 else str(x)[:-2])\n",
    "nytcovid[\"date\"] = pd.to_datetime(nytcovid[\"date\"], format='%Y-%m-%d')\n",
    "test = test.rename(columns={'date_dt': 'date'})\n",
    "test = test.merge(nytcovid, how=\"left\", on=['date', 'fips_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gb_date = test.groupby([\"date\"])\n",
    "#ratio over time\n",
    "summary = pd.DataFrame()\n",
    "summary[\"ratio\"] = test_gb_date.different_fips.mean() / test_gb_date.total_contacts.mean()\n",
    "#summary[\"deaths\"] = test_gb_date.deaths.mean()\n",
    "plt.figure()\n",
    "summary.plot(figsize=(10,10))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('out/in county ratio')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(\"Ratio of In vs Out of County Contacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame()\n",
    "summary[\"total_contacts\"]= test_gb_date.total_contacts.mean()\n",
    "summary[\"different_fips\"] = test_gb_date.different_fips.mean()\n",
    "summary[\"att_packer\"] = test_gb_date.at_packer.mean()\n",
    "#summary[\"deaths\"] = test_gb_date.deaths.mean()\n",
    "plt.figure()\n",
    "summary.plot(figsize=(10,10))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('# Contacts')\n",
    "#plt.legend(loc='upper left')\n",
    "\n",
    "plt.title(\"Average Contacts In vs Out of Packer County\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame()\n",
    "summary[\"deaths\"] = test_gb_date.deaths.mean()\n",
    "summary[\"cases\"] = test_gb_date.cases.mean()\n",
    "plt.figure()\n",
    "summary.plot(figsize=(10,10))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of People')\n",
    "#plt.legend(loc='upper left')\n",
    "\n",
    "plt.title(\"Deaths vs Cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
